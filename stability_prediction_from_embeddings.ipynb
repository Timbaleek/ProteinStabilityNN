{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Protein stability prediction\n",
    "\n",
    "In the project you will try to predict protein stability changes upon point mutations. \n",
    "We will use acuumulated data from experimental databases, i.e. the Megascale dataset. A current [pre-print paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10402116/) has already preprocessed the dataset and created homology reduced data splits. We will reuse these. To do so, download the data folder from [here](https://polybox.ethz.ch/index.php/s/txvcb5jKy1A0TbY) and unzip it.  \n",
    "\n",
    "The data includes measurements of changes in the Gibbs free enrgy ($\\Delta \\Delta G $). \n",
    "This will be the value that you will have to predict for a given protein with a point mutation. \n",
    "As input data you can use the protein sequence or a protein embedding retreived from ESM, a state of the art protein model.  \n",
    "\n",
    "Here we will use protein embeddings computed by ESM as input. \n",
    "We provide precomputed embeddings from the last layer of the smallest ESM model. You can adjust the Dataloader's code to load the embedding of the wild type or of the mutaed sequence or both. You can use it however you like. This is just to provide you easy access to embeddings. If you want to compute your own embeddings from other layers or models you can do that, too. \n",
    "\n",
    "Below we provide you with a strcuture for the project that you can start with.  \n",
    "Edit the cells to your liking and add more code to create your final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn.metrics as skmetrics\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import lightning as L\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.regression import PearsonCorrCoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading\n",
    "\n",
    "We are using the Megascale dataset. The train, validation and test sets are already predefined.  \n",
    "As mentioned, we provide embeddings from the last layer of ESM as input. You can access either the wild type or the mutated sequence and you could also further adjsut the embeddings. \n",
    "Here we have an embedding representing the complete sequence. It was computed by averaging over the embeddings per residue in the sequence. \n",
    "\n",
    "The ``Dataset`` classes return tuples of ``(embedding, ddg_value)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataloaders load the tensors from memory one by one, could potentially become a bottleneck\n",
    "\n",
    "class ProtEmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for the embeddings of the mutated sequences\n",
    "    You can the get_item() method to return the data in the format you want\n",
    "    \"\"\"\n",
    "    def __init__(self, tensor_folder, csv_file, id_col=\"name\", label_col=\"ddG_ML\"):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        input at init: \n",
    "            tensor_folder: path to the directory with the embeddings we want to use, eg. \"/home/data/mega_train_embeddings\"\n",
    "            cvs_file: path to the csv file corresponding to the data, eg. \"home/data/mega_train.csv\"\n",
    "        \"\"\"\n",
    "        self.tensor_folder = tensor_folder\n",
    "        self.df = pd.read_csv(csv_file, sep=\",\")\n",
    "        # only use the mutation rows\n",
    "        self.df = self.df[self.df.mut_type!=\"wt\"]\n",
    "        # get the labels and ids\n",
    "        self.labels = torch.tensor(self.df[label_col].values)\n",
    "        self.ids = self.df[id_col].values\n",
    "        self.wt_names = self.df[\"WT_name\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load embeddings\n",
    "        # mutation embedding\n",
    "        tensor_path = os.path.join(self.tensor_folder, self.ids[idx] + \".pt\")\n",
    "        tensor = torch.load(tensor_path)['mean_representations'][6]\n",
    "\n",
    "        # wildtype embedding, uncomment if you want to use this, too\n",
    "        #tensor_path_wt = os.path.join(self.tensor_folder, self.wt_names[idx] + \".pt\")\n",
    "        #tensor_wt = torch.load(tensor_path_wt)['mean_representations'][6]\n",
    "\n",
    "        label = self.labels[idx] # ddG value\n",
    "        # returns a tuple of the input embedding and the target ddG values\n",
    "        return tensor, label.float()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/course/ProteinStabilityNN'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#goOneUp()\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#go one directory up\n",
    "def goOneUp():\n",
    "    path_parent = os.path.dirname(os.getcwd())\n",
    "\n",
    "    os.chdir(path_parent)\n",
    "    os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage \n",
    "# make sure to adjust the paths to where your files are located\n",
    "dataset_train = ProtEmbeddingDataset('project_data/project_data/mega_train_embeddings', 'project_data/project_data/mega_train.csv')\n",
    "dataset_val = ProtEmbeddingDataset('project_data/project_data/mega_val_embeddings', 'project_data/project_data/mega_val.csv')\n",
    "dataset_test = ProtEmbeddingDataset('project_data/project_data/mega_test_embeddings', 'project_data/project_data/mega_test.csv')\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1024, shuffle=True, num_workers=16)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=512, shuffle=False, num_workers=16)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture and training\n",
    "\n",
    "Now it's your turn. Create a model trained on the embeddings and the corresponding ddG values.  \n",
    "Be aware that this is not a classification task, but a regression task. You want to predict a continuous number that is as close to the measured $\\Delta \\Delta G $ value as possible.\n",
    "You will need to adjust your architecture and loss accordingly.\n",
    "\n",
    "Train the model with the predefined dataloaders. And try to improve the model. \n",
    "Only test on the test set at the very end, when you have finished fine-tuning you model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset_test[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.3946e-01,  3.0778e-01, -6.1013e-01, -1.3587e+00, -3.3663e-01,\n",
       "          4.5446e-01,  4.3775e-01,  8.9823e-02, -1.2032e+00,  6.7619e-02,\n",
       "         -1.8480e-01, -1.5515e-01,  3.3124e-01,  4.6695e-01, -8.8386e-02,\n",
       "         -4.5464e-01,  1.4459e-01, -3.2557e-01, -5.2961e-02, -6.9701e-02,\n",
       "         -9.3391e-02, -8.2242e-02, -4.5464e-01,  1.2382e-01,  5.3554e-01,\n",
       "          6.2861e-01, -4.7345e-01, -7.4468e-01,  4.7029e-01, -7.2735e-01,\n",
       "          6.0254e-01, -4.9718e-01, -7.4361e-02,  9.7470e-01, -2.1294e-01,\n",
       "          8.5849e-01,  2.0630e-01, -1.4643e+00, -1.7932e-01,  1.0862e-01,\n",
       "         -3.9089e-02,  1.1207e+00,  6.4455e-03, -3.1676e-01,  1.1554e-01,\n",
       "          5.3930e-02, -8.9529e-01,  2.2434e-01, -3.2585e-01,  3.5273e-01,\n",
       "          9.9661e-01, -2.8097e-01,  8.1179e-01,  1.2510e+00, -3.3640e-01,\n",
       "          6.1254e-01,  2.7245e-01,  1.1272e-01, -4.1809e-01, -7.5295e-02,\n",
       "          1.1075e-01, -1.0476e-01,  4.9461e-02,  1.1199e+00,  4.0499e-01,\n",
       "         -4.0259e-01,  3.2493e-01,  5.7523e-01,  1.4210e+00,  1.8410e-01,\n",
       "         -1.0777e-01,  9.6413e-02, -4.7555e-01,  1.7834e-01,  4.8738e-03,\n",
       "          7.2233e-02,  7.2874e+00, -8.8422e-01,  2.9594e-01, -6.5174e-01,\n",
       "          2.0820e-03, -9.6501e-01,  4.2810e-03,  1.5520e-01, -2.5031e-01,\n",
       "          1.5165e-01, -1.5683e+00, -8.4055e-02,  2.8917e-01,  4.8563e-03,\n",
       "          5.1326e-01,  3.5149e-01, -2.1856e-01, -1.1759e-01, -4.1810e-01,\n",
       "          1.6717e-01, -2.0986e-01,  7.7679e-01, -3.0474e-01, -2.2344e-01,\n",
       "          7.1422e-01,  3.7492e-01, -3.7956e-01, -1.2168e+00, -6.6576e-01,\n",
       "         -2.7174e-01, -3.8775e-01,  5.5594e-01,  5.8412e-02, -2.1591e-02,\n",
       "          1.1294e-02,  4.8514e-01,  9.1567e-02,  5.6372e-01,  3.5507e-01,\n",
       "         -5.6219e-01, -2.9333e-01,  1.7356e-01,  2.9613e-01,  3.6521e-01,\n",
       "         -5.0110e-01, -6.8359e-01, -4.3816e-01,  7.8203e-01,  5.0926e-01,\n",
       "         -1.0227e+00, -2.4184e-02,  6.1680e-01, -2.3035e-01, -5.4428e-01,\n",
       "          8.1753e-01,  1.6928e-01,  1.1118e+00, -1.4670e-01, -3.2040e-01,\n",
       "         -1.4167e-01,  7.9237e-01, -1.4664e-01,  6.7650e-01,  4.7020e-01,\n",
       "         -7.3734e-02, -1.5909e-01, -4.6251e-01, -1.8327e-01, -2.1331e-01,\n",
       "         -1.1820e+00, -8.3547e-01,  8.5295e-01, -1.6324e-01,  4.1300e-01,\n",
       "          3.8407e-01, -4.3606e-02,  1.0236e-01, -1.3550e-01,  7.7772e-01,\n",
       "          3.6259e-02, -2.7165e-01,  1.3084e-02, -3.8439e-01,  6.4937e+00,\n",
       "         -5.5410e-01, -1.5462e-01, -1.6469e+00, -3.2154e-01, -1.4236e-01,\n",
       "         -5.9797e-01,  1.7372e+00, -1.9918e-01,  1.6308e-02, -3.2786e-01,\n",
       "         -6.3608e-01, -2.8889e-01, -9.0386e-01,  2.5756e-01,  2.1002e+00,\n",
       "          3.8030e-01,  3.4915e-03,  5.8744e-01,  7.9928e-01, -1.1901e-01,\n",
       "         -2.0739e-02,  1.0422e-01,  7.5578e-02,  1.6332e-01, -2.5810e-01,\n",
       "         -7.9816e-01,  2.1186e-01,  1.9216e+00,  1.0685e-01, -4.5707e-01,\n",
       "         -1.8779e+00,  2.8562e-01,  5.1498e-01, -3.8273e-01,  1.0383e+00,\n",
       "          7.4257e-01,  2.2715e-01,  6.0793e-02,  1.6740e-01, -2.0848e-01,\n",
       "          8.0014e-01, -2.9055e-01,  2.1992e-01,  4.3408e-01,  7.9414e-01,\n",
       "         -3.7754e-02,  2.3319e-02,  4.9183e-01,  2.8546e-02, -1.0784e-01,\n",
       "         -6.3507e-01, -1.0037e+00, -5.4348e-01, -6.8932e-01,  3.2226e-02,\n",
       "         -2.6191e-01, -4.2289e-01, -4.7330e-01,  1.0021e+00, -5.7448e-01,\n",
       "         -3.5152e-01, -6.6104e-01, -8.3178e-02, -1.9811e-01,  2.0641e-01,\n",
       "          1.5403e+00,  5.0402e-01,  6.0490e-01,  4.6042e-01, -3.8999e-01,\n",
       "         -1.9359e-01, -2.0898e-02,  1.7722e-01,  3.8438e-01, -4.5728e-01,\n",
       "         -1.2579e+00,  2.9856e-01, -3.1844e-01,  1.6584e-01,  4.1234e-01,\n",
       "         -1.5282e-01,  3.9113e-01,  2.9076e-01,  2.6114e-01,  1.0611e-01,\n",
       "          3.3234e-01,  3.3593e-01,  7.8345e-02, -1.4402e-01, -4.8042e-01,\n",
       "         -7.0542e-01,  4.4847e-01, -7.7221e-02,  1.3055e-01,  2.2168e-01,\n",
       "         -1.9247e-01,  3.0813e-01,  4.8018e-01,  7.2738e-01, -2.0804e+01,\n",
       "          7.4392e-01, -1.4377e-01, -2.6888e-01,  9.6241e-02, -7.6025e-01,\n",
       "         -4.1530e-01, -1.4266e+00,  1.4683e+00, -9.2748e-01, -1.6907e-01,\n",
       "          2.9864e-01, -5.0979e-01, -1.3968e-01, -5.1555e-01,  3.7583e-01,\n",
       "          3.3192e-03,  6.9358e-01, -4.5030e-01,  1.9519e-01,  1.7086e-01,\n",
       "         -1.7522e+00,  5.2834e-01, -1.7202e-01,  3.0959e-01, -2.8303e-01,\n",
       "         -7.3856e-02, -4.4069e-01,  4.2051e-01,  8.1764e-02,  4.0113e-01,\n",
       "          3.5921e-01,  3.5597e-01, -6.3213e-01, -1.1197e-01,  9.2986e-01,\n",
       "          4.7748e-01,  4.7371e-01, -2.8559e-01, -1.5368e-01,  8.6432e-02,\n",
       "          7.7380e-01,  8.6270e-01,  5.4595e-02, -8.4261e-01, -6.6540e-01,\n",
       "         -2.9379e-01, -2.5098e-01,  2.2286e-01, -2.8661e-01,  1.6176e-01,\n",
       "          3.6934e-01, -1.3937e+00, -3.1973e-01, -3.6511e-01,  2.5743e-02,\n",
       "         -1.5386e+00,  7.2233e-02,  7.5487e-01,  7.3544e-01,  4.6769e-01,\n",
       "         -3.0430e-01, -8.5456e-01, -7.0047e-01, -2.9952e-01, -4.9515e-01,\n",
       "          3.7720e-02, -9.0779e-01, -5.1188e-01, -1.3414e+00,  1.9567e-01,\n",
       "         -1.6028e-01, -5.0281e-01,  5.0035e-01, -1.6780e-01, -1.2488e-01,\n",
       "         -2.0933e-02,  4.1702e-01,  7.0666e-01,  1.0537e-01,  3.0854e-01,\n",
       "         -4.0772e-01, -5.8082e-01,  2.1522e-01,  2.2603e-01, -4.9788e-01,\n",
       "         -2.8695e-01,  3.7483e-01, -1.1996e+00,  1.1639e-01, -1.0042e-01,\n",
       "         -1.7953e+00,  2.0905e-01,  3.1382e-01, -1.8762e+00, -1.2618e-01,\n",
       "         -5.7147e-01,  9.0486e-01, -1.4671e+00, -5.2220e-02,  7.5626e-02,\n",
       "         -4.3062e-01,  1.1284e-01, -1.1235e+00, -2.9219e+00, -1.8876e-01,\n",
       "          2.0768e-01, -1.3247e+00, -4.2910e-01,  3.1185e-01,  3.8512e-01,\n",
       "         -1.8984e-01, -1.9886e-01, -2.8572e-01, -5.4006e-02, -3.8127e-01,\n",
       "          1.5722e-01, -2.3788e-02, -1.6069e-01,  1.7281e+00, -4.2972e-01,\n",
       "          6.0753e-02, -8.1121e-02,  1.1865e-01,  6.0180e-01,  2.7485e+01,\n",
       "          6.5942e-01,  1.6380e+00,  4.3262e-01, -1.5074e-01, -2.6410e-02,\n",
       "         -6.4111e-02, -5.4150e-01, -3.9975e-01, -4.2795e-01, -1.8081e-01,\n",
       "         -1.7627e-01,  1.6021e-01,  4.9039e-01, -2.3257e-01, -4.2802e-01,\n",
       "          5.1174e-01,  4.4519e-01,  1.7573e-01, -3.0074e-01,  7.7537e-01,\n",
       "         -1.7024e-02,  9.3683e-02, -3.9633e-02, -5.1647e-01, -1.5119e-01,\n",
       "         -5.2963e-01, -3.2826e-01, -4.8724e-01, -3.0296e-01, -7.7771e-01,\n",
       "          2.8836e-01, -5.2189e-01,  1.6487e+00,  4.6123e-01,  2.3031e-01,\n",
       "         -7.5869e-01, -1.0771e+00, -3.1905e-01, -1.2012e+00,  7.1138e-01,\n",
       "         -7.6859e-02, -5.3413e-01,  3.6501e-01, -8.4208e-02, -4.7520e-01,\n",
       "         -1.2215e+00, -3.0362e-01, -2.0113e-01, -5.4727e-01,  1.7502e-01,\n",
       "         -9.1148e-01, -1.4420e+00,  2.9714e-01, -7.4909e-01, -9.3192e-02,\n",
       "          4.2707e-01, -1.8690e-01,  1.0503e-01,  1.9023e-01, -4.3218e-01,\n",
       "          2.7669e-01, -2.8704e-02,  4.0696e-02,  6.3247e-01, -1.0213e-01,\n",
       "          1.5758e-01, -1.3640e+00,  6.4810e-01,  5.4751e-02,  1.8644e-02,\n",
       "         -9.3234e-01, -2.0161e-01, -5.2532e-01,  8.5259e-01,  1.2606e-01,\n",
       "          6.4373e-01, -8.5825e-01,  3.1724e-01,  5.0673e-01, -1.6139e+00,\n",
       "         -4.7259e-01, -2.5406e-02, -4.9651e-01,  3.6741e-01,  7.0142e-01,\n",
       "         -6.1584e-02, -2.7908e-01,  9.0215e-01,  4.8743e-01,  7.2140e-01,\n",
       "         -2.6749e-01, -4.5610e-01, -1.0508e+00, -6.6775e-01, -3.1530e-01,\n",
       "         -3.9996e-02, -1.9925e-01, -3.8684e-01, -6.6521e-01, -3.3946e-01,\n",
       "          2.5458e-01, -1.4517e-01,  3.1247e-01,  2.4479e-01, -2.4639e-01,\n",
       "          1.9508e-02, -7.8235e-01, -2.6665e-01,  1.2172e+00, -3.2180e-01,\n",
       "          1.1222e-01,  2.2322e-01, -3.0677e-01, -3.3293e-01, -4.4005e-01,\n",
       "         -2.0646e-01, -1.0019e-01, -1.2220e-01,  8.5102e-01,  1.0175e+00,\n",
       "         -3.8878e-01, -3.6714e-01, -1.8616e-02, -1.5251e-01, -1.4130e+00,\n",
       "         -2.8454e-01, -2.8742e-01,  9.5526e-01,  5.1012e-01, -8.6075e-01,\n",
       "          6.9984e-01, -8.3364e-02, -3.3847e-01, -1.7907e-01,  4.5675e-01,\n",
       "          8.2237e-01, -3.5906e-01,  7.7845e-01,  2.4390e-01, -7.9755e-01,\n",
       "         -3.5405e-02,  1.1868e-01,  1.1582e-01,  4.3934e-01,  6.7263e-01,\n",
       "         -7.2044e-02,  1.9592e-01, -5.4485e-01, -5.1327e-02, -1.2700e+00,\n",
       "         -1.0051e+00, -6.7596e-02, -7.3079e-01,  1.0657e-01,  8.8170e-01,\n",
       "         -3.9063e-01, -5.8786e-01, -2.2109e-01, -1.8461e-01, -2.9411e-02,\n",
       "          5.5032e-01,  4.4010e-01,  1.4696e-01,  6.8006e-01, -2.0149e-01,\n",
       "          6.6606e-03,  4.2048e-01,  1.5676e-01, -8.1531e-01,  9.9759e-01,\n",
       "          1.1406e-03, -5.9232e-01,  7.4868e-01, -1.3060e-01, -2.4458e-01,\n",
       "          5.3531e-01, -6.3577e-01, -9.6097e-01, -6.2950e-01, -1.5755e-01,\n",
       "          1.4341e-01,  2.1912e-01, -1.1561e+00,  3.2593e-04,  2.2450e-01,\n",
       "         -1.6715e-01, -1.1617e-01, -2.8654e-01,  5.5991e-01, -4.3680e-01,\n",
       "          6.0505e-01,  5.5696e-01, -4.9155e-01, -1.0119e+00, -1.7759e+00,\n",
       "         -1.7423e-01,  8.5799e-02,  4.0937e-01,  6.0001e-01, -8.1001e-01,\n",
       "          1.4716e+00, -3.0171e-01,  7.1548e-01,  3.2002e-01,  1.4475e+00,\n",
       "         -8.6665e-02,  1.0962e+00, -4.0310e-02,  1.1566e-01, -5.6344e-01,\n",
       "          5.7718e-01, -6.9900e-01, -4.6261e-01,  2.3510e-01,  1.7703e+00,\n",
       "          1.5912e-02, -4.6966e-01, -1.3233e+00, -1.2140e-01, -1.5082e-01,\n",
       "          7.3428e-01, -5.6280e-01, -1.8421e-01,  1.0792e+00, -3.8078e-01,\n",
       "          3.2242e-01,  1.1120e-01, -6.1885e-01,  2.8287e-01, -6.0181e-01,\n",
       "          3.8005e-01,  2.0213e-01, -9.9288e-02,  3.7664e-01, -5.4019e-01,\n",
       "         -9.6573e-01, -3.0782e-01,  2.0479e-01,  3.9819e-01,  9.5687e-01,\n",
       "          9.0436e-02,  7.5535e-01,  1.6312e-02,  3.6977e-01, -4.0254e-01,\n",
       "          5.2819e-01, -9.6812e-01, -3.3268e-01,  5.2804e-01, -4.7706e-01,\n",
       "          4.4640e-01,  3.4664e-02,  3.0006e-01,  3.2938e-01,  7.8721e-01,\n",
       "         -1.8073e-01,  4.0269e-01,  2.2693e+00,  1.7875e-01,  1.8165e-01,\n",
       "          3.8272e-02, -1.0430e-01,  1.3539e-01, -1.2329e+00,  2.3973e-01,\n",
       "         -2.5448e-01,  1.4697e-01,  5.8760e-02,  1.6293e-01,  3.5113e-01,\n",
       "         -2.7865e-01, -4.0396e-01,  1.0185e+00,  1.3974e+00, -1.8486e-01,\n",
       "         -9.4466e-02,  2.8059e-01,  2.0079e-01,  4.4438e-01, -2.8080e-01,\n",
       "          2.2172e-01, -6.6667e-01, -3.3905e-01, -6.7240e-01,  7.8473e-01,\n",
       "          5.6585e-01,  3.7216e+00,  2.7334e-01, -7.0423e-02, -1.2305e-01,\n",
       "          3.2412e-01, -3.4001e-01, -1.2168e+00, -1.8654e-01,  3.0428e-01,\n",
       "          1.0312e+00, -4.3813e-01,  2.7192e-01, -7.8112e-01,  1.7936e-01,\n",
       "          6.1910e-01,  1.5983e-01,  7.4882e-01,  8.5931e-01, -1.6612e-01,\n",
       "         -2.3132e-01, -3.2494e-01, -5.2098e-01, -1.9307e-01,  1.0872e-01,\n",
       "         -3.7205e-01,  7.7336e-02,  1.3831e-01, -1.2840e-01,  5.5253e+00,\n",
       "         -8.8471e-01, -2.9276e-01,  6.3460e+00, -1.7509e-01, -5.1547e-02,\n",
       "          3.2973e-01,  2.0924e+00, -1.4919e-01,  7.1022e-01, -1.3510e+00,\n",
       "         -7.1331e-02, -4.0619e-01,  4.4168e-01, -3.7331e-01,  2.9442e-01,\n",
       "         -2.2882e-01, -8.2945e-03, -1.4192e-01, -1.9280e-01, -3.9614e-01,\n",
       "         -2.8033e-02,  5.5129e-01,  1.1996e-01, -3.4036e-01, -2.0787e-01,\n",
       "          8.8846e-02, -5.3241e-01,  7.9257e-01, -6.8468e-01,  5.8478e-01,\n",
       "          3.4678e-01,  3.8827e-01, -6.7052e-01, -3.2965e-02,  2.8834e-01,\n",
       "          1.5139e-01, -3.1699e-01, -3.1866e-01, -4.9108e-03, -1.2915e-01,\n",
       "         -2.0325e-01,  2.7702e-03, -2.3996e-01, -6.4159e-02, -6.7359e-01,\n",
       "          4.8590e-01, -6.1283e-01,  1.9542e+00, -1.0540e-01,  9.0105e-01,\n",
       "          5.7361e-03, -3.2558e-01,  2.0611e-02, -8.0688e-02,  1.8429e+00,\n",
       "         -1.4939e+00,  1.0108e+00,  1.1117e+00, -4.8747e-01, -6.7128e-01,\n",
       "          2.4182e-01,  7.8502e-01, -2.7507e-01,  2.1734e-01, -5.1338e-01,\n",
       "          6.0902e-01,  7.6495e-01,  2.9728e-01]),\n",
       " tensor(-0.2959))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.__getitem__(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the output shape of our data after a convolution and pooling of a certain size\n",
    "\n",
    "def get_conv2d_out_shape(tensor_shape, conv, pool=2):\n",
    "    # return the new shape of the tensor after a convolution and pooling\n",
    "    # tensor_shape: (channels, height, width)\n",
    "    # convolution arguments\n",
    "    kernel_size = conv.kernel_size\n",
    "    stride=conv.stride # 2D array\n",
    "    padding=conv.padding # 2D array\n",
    "    dilation=conv.dilation # 2D array\n",
    "    out_channels = conv.out_channels\n",
    "\n",
    "    height_out = np.floor((tensor_shape[1]+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n",
    "    #width_out = np.floor((tensor_shape[2]+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n",
    "    \n",
    "    if pool:\n",
    "        # adjust dimensions to pooling\n",
    "        height_out/=pool\n",
    "        #width_out/=pool\n",
    "        \n",
    "    return int(out_channels),int(height_out)#,int(width_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "class embedConvModel(nn.Module):\n",
    "    \n",
    "    # Network Initialisation\n",
    "    def __init__(self, params):\n",
    "        \n",
    "        super(embedConvModel, self).__init__() #initialize parent pytorch module\n",
    "\n",
    "        # read parameters\n",
    "        inputShape = params[\"inputShape\"]\n",
    "        kernelSize = params[\"kernelSize\"]\n",
    "        \n",
    "        #channels_out = params[\"initial_depth\"] \n",
    "        #fc1_size = params[\"fc1_size\"]\n",
    "        \n",
    "        #### Convolution Layers\n",
    "\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        ##conv layer 1\n",
    "        # convolution with kernel size 8, goes from three channels to \n",
    "        # number defined by initial_depth in params\n",
    "        self.conv1 = nn.Conv1d(inputShape[0], inputShape[0], kernel_size=kernelSize[0])#, padding=int((kernelSize[0]-1)/2), padding_mode='zeros')\n",
    "        #shape in = (3, 256, 256)\n",
    "        print(inputShape)\n",
    "        outShape = get_conv2d_out_shape(inputShape, self.conv1, pool=2)\n",
    "        print(outShape)\n",
    "        flatSize = outShape[1]# * outShape[2] #768 * 1024\n",
    "        print(flatSize)\n",
    "        self.fc = nn.Linear(flatSize, 1024)#outShape[1])\n",
    "\n",
    "    def forward(self,X):\n",
    "        # our network's forward pass\n",
    "        \n",
    "        # Convolution & Pool Layers\n",
    "        ############# TODO ###############\n",
    "        # convolution (conv1), then relu, then max pool \n",
    "        X = F.tanh(self.conv1(X))\n",
    "        X = self.pool(X)\n",
    "        X = torch.flatten(X, 1)\n",
    "        \n",
    "\n",
    "        #####################################\n",
    "        # return log softmax to fit classification problem, no relu needed\n",
    "        return self.fc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitMRIModel(L.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        ######## TODO ##########\n",
    "        # pass our model \n",
    "        self.model = model\n",
    "        #pass the learning rate\n",
    "        self.lr = learning_rate\n",
    "        # define loss function\n",
    "        self.loss_function = nn.MSELoss() #TODO\n",
    "        # define accuracy metric (torchmetrics)\n",
    "        #self.accuracy = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        ########################\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        ######### TODO #############\n",
    "        \n",
    "        # read from batch\n",
    "        # TODO\n",
    "        x,y = batch\n",
    "        x = torch.unsqueeze(x,1)\n",
    "        print(x)\n",
    "        print(y)\n",
    "        # run data through model\n",
    "        predictions = self.model(x)#.squeeze(1)\n",
    "        print(predictions.shape)\n",
    "        # compute loss\n",
    "        # 1 prediction\n",
    "        loss = self.loss_function(predictions, y)\n",
    "        #loss = self.loss_function(predictions, y)\n",
    "        # compute accuracy\n",
    "        #acc = self.accuracy(predictions, y)\n",
    "        ##############################\n",
    "\n",
    "        # logging the values (will appear in progress bar and on dashboard)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        #self.log(\"train_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        ############## TODO ################\n",
    "        # define the optimizer, let's use Adam\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        ####################################\n",
    "        return optimizer\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "\n",
    "        ############### TODO #############\n",
    "        # read from batch\n",
    "        x,y = batch\n",
    "\n",
    "        x = torch.unsqueeze(x,1)\n",
    "        # run data through model\n",
    "        predictions = self.model(x)#.squeeze(1)\n",
    "        \n",
    "        # compute loss\n",
    "        #loss = loss(prediction,labels[mask==1])\n",
    "        loss = self.loss_function(predictions, y)\n",
    "        # compute accuracy\n",
    "        #acc = self.accuracy(predictions, y)\n",
    "        ##############################\n",
    "\n",
    "        # logging\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n",
    "        return loss#, acc\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        ############### TODO #############\n",
    "        # read from batch\n",
    "        x,y = batch\n",
    "\n",
    "        x = torch.unsqueeze(x,1)\n",
    "        print(x.shape)\n",
    "        #print(x.shape)\n",
    "        #print(y.shape)\n",
    "        # run data through model\n",
    "        predictions = self.model(x)#.squeeze(1)\n",
    "        #print(predictions.shape)\n",
    "        # compute loss\n",
    "        loss = self.loss_function(predictions, y)\n",
    "        #loss = self.loss_function(predictions, y)\n",
    "        # compute accuracy\n",
    "        #acc = self.accuracy(predictions, y)#torchmetrics.classification.Accuracy(task=\"MULTICLASS\", num_classes=2)\n",
    "        ##############################\n",
    "\n",
    "        # logging\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        #self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024, 768)\n",
      "(1, 509)\n",
      "509\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "params_model={\n",
    "    \"inputShape\": (1, batch[0].shape[0], batch[0].shape[1]),\n",
    "    \"kernelSize\": (7,1)\n",
    "    }\n",
    "\n",
    "# define computation hardware approach (GPU/CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Instantiate the model\n",
    "cnn_model = embedConvModel(params_model)\n",
    "# moves the model to GPU if available\n",
    "cnn_model = cnn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 768])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 768])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(batch[0],0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type           | Params\n",
      "-------------------------------------------------\n",
      "0 | model         | embedConvModel | 522 K \n",
      "1 | loss_function | MSELoss        | 0     \n",
      "-------------------------------------------------\n",
      "522 K     Trainable params\n",
      "0         Non-trainable params\n",
      "522 K     Total params\n",
      "2.089     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cf6ec704db44d4b3585b5a20ac5730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (512x381 and 509x1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#, callbacks=[FineTuneLearningRateFinder(milestones=(5, 10))])\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, dataloader_train, dataloader_val)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    545\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1030\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1030\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1032\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1059\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1056\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1059\u001b[0m val_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1061\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, hook_name, \u001b[38;5;241m*\u001b[39mstep_args)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[110], line 84\u001b[0m, in \u001b[0;36mLitMRIModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#print(y.shape)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# run data through model\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\u001b[38;5;66;03m#.squeeze(1)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#print(predictions.shape)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# compute loss\u001b[39;00m\n\u001b[1;32m     87\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(predictions, y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[109], line 45\u001b[0m, in \u001b[0;36membedConvModel.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     40\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(X, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#####################################\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# return log softmax to fit classification problem, no relu needed\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (512x381 and 509x1024)"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "########## TODO #############\n",
    "# instantiate lightning model with the cnn_model and learning_rate=1e-3\n",
    "model = LitMRIModel(cnn_model, learning_rate=1e-3)\n",
    "############################\n",
    "\n",
    "# instantiate the lightning trainer \n",
    "trainer = L.Trainer(max_epochs=20, log_every_n_steps=1)#, callbacks=[FineTuneLearningRateFinder(milestones=(5, 10))])\n",
    "# train\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and visualization\n",
    "\n",
    "To get a good feeling of how the model is performing and to compare with literature, compute the Pearson and Spearman correlations.\n",
    "You can also plot the predictions in a scatterplot. We have added some code for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds =[]\n",
    "all_y = []\n",
    "# save all predictions\n",
    "for batch in dataloader_val:\n",
    "    # adjust this to work with your model\n",
    "    x,y = batch\n",
    "    y_hat = model(x)\n",
    "    preds.append(y_hat.squeeze().detach().numpy())\n",
    "    all_y.append(y.detach().numpy())\n",
    "\n",
    "# concatenate and plot\n",
    "preds= np.concatenate(preds)\n",
    "all_y = np.concatenate(all_y)\n",
    "\n",
    "sns.regplot(x=preds,y=all_y)\n",
    "plt.xlabel(\"Predicted ddG\")\n",
    "plt.ylabel(\"Measured ddG\")\n",
    "\n",
    "# get RMSE, Pearson and Spearman correlation \n",
    "print(\"RMSE:\", skmetrics.mean_squared_error(all_y, preds, squared=\"False\"))\n",
    "print(\"Pearson r:\", scipy.stats.pearsonr(preds, all_y))\n",
    "print(\"Spearman r:\", scipy.stats.spearmanr(preds, all_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fa888489dcef296c36d3d3b759d2bbafdf14549bdfa862d5619a4427d05b08f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
